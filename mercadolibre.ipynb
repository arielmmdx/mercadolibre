{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceab8d77-808d-4912-8ea6-3dc840e5c300",
   "metadata": {},
   "source": [
    "# Install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95d7d67-7e31-469d-a7a1-16c733d292b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/glue_user/.local/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/glue_user/.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/glue_user/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/glue_user/.local/lib/python3.10/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/glue_user/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694f083-7b51-4e4a-922e-b3a29a98e4ea",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259aca0a-1d2d-48fe-84cd-3753844b12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 21:15:39,327 - INFO - Libraries calls ok\n",
      "2024-06-10 21:15:39,329 - INFO - Start Time : 20240610_211539 \n",
      "2024-06-10 21:15:39,340 - INFO - Reading prints.json\n",
      "2024-06-10 21:15:44,741 - INFO - Reading taps.json\n",
      "2024-06-10 21:15:45,172 - INFO - Reading pays.csv\n",
      "2024-06-10 21:15:45,533 - INFO - Transform data and saving output dataset in output3.csv\n",
      "2024-06-10 21:15:45,534 - INFO - Starting data processing\n",
      "2024-06-10 21:15:45,535 - INFO - Stripping whitespace from 'value_prop' columns\n",
      "2024-06-10 21:15:45,849 - INFO - Calculating the last print date and last week start date\n",
      "2024-06-10 21:15:45,853 - INFO - Filtering prints for the last week\n",
      "2024-06-10 21:15:45,867 - INFO - Merging taps data with prints to add clicked column\n",
      "2024-06-10 21:15:45,986 - INFO - Adding columns print_date to df_prints_last_week\n",
      "2024-06-10 21:15:45,988 - INFO - number of times the user viewed each value prop in the 3 weeks prior to each print.\n",
      "2024-06-10 21:15:46,319 - INFO - Aggregating data based on views_3w in the last three weeks\n",
      "2024-06-10 21:15:46,364 - INFO - number of times the user clicked each value prop in the 3 weeks prior to each print.\n",
      "2024-06-10 21:15:46,438 - INFO - Aggregating data based on clicks_3w in the last three weeks\n",
      "2024-06-10 21:15:46,448 - INFO - number of payments that the user made for each value prop in the 3 weeks prior to each print.\n",
      "2024-06-10 21:15:46,800 - INFO - Aggregating data based on payments_3w and total_spent_3w in the last three weeks\n",
      "2024-06-10 21:15:46,841 - INFO - Merging aggregated data with prints\n",
      "2024-06-10 21:15:47,012 - INFO - Selecting required columns and filling NAs with 0\n",
      "2024-06-10 21:15:47,049 - INFO - Saving processed DataFrame to expected_result.csv\n",
      "2024-06-10 21:15:49,079 - INFO - Processed DataFrame saved\n",
      "2024-06-10 21:15:49,168 - INFO - End Time : 20240610_211549 \n",
      "2024-06-10 21:15:49,169 - INFO - Duration: 0:00:09.839569\n",
      "2024-06-10 21:15:49,171 - INFO - PROCESS_EXECUTED_SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "DTYPE_PRINTS = {\n",
    "    'day': 'datetime64[ns]',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_TAPS = {\n",
    "    'day': 'datetime64[ns]',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_PAYS = {\n",
    "    'pay_date': 'datetime64[ns]',\n",
    "    'total': 'float',\n",
    "    'user_id': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_EXPECTED_RESULTS = {\n",
    "    'user_id': 'int',\n",
    "    'day': 'datetime64[ns]',\n",
    "    'value_prop': 'str',\n",
    "    'clicked': 'bool',\n",
    "    'views_3w': 'int',\n",
    "    'clicks_3w': 'int',\n",
    "    'payments_3w': 'int',\n",
    "    'total_spent_3w': 'float'\n",
    "}\n",
    "\n",
    "PRINTS_PATH = 'prints.json'\n",
    "TAPS_PATH = 'taps.json'\n",
    "PAYS_PATH = 'pays.csv'\n",
    "EXPECTED_RESULT_PATH = \"expected_result.csv\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Libraries calls ok\")\n",
    "START_TIME = datetime.now()\n",
    "logging.info(f\"Start Time : {START_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "\n",
    "def read_json_lines_to_dataframe(json_lines_path, dtypes):\n",
    "    \"\"\"\n",
    "    Builds a pandas DataFrame from JSON lines with specified column names and data types.\n",
    "    \n",
    "    Parameters:\n",
    "    json_lines_path (str): Path to a file containing JSON lines.\n",
    "    dtypes (dict): Dictionary mapping column names to their expected data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the specified columns and data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_lines_path, 'r') as file:\n",
    "            json_lines_str = file.read()\n",
    "\n",
    "        json_lines = [json.loads(line) for line in json_lines_str.strip().split('\\n')]\n",
    "        df = pd.json_normalize(json_lines)\n",
    "\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'day': columns[0],\n",
    "            'user_id': columns[1],\n",
    "            'event_data.position': columns[2],\n",
    "            'event_data.value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.info(f\"Error: The file at {json_lines_path} was not found.\")\n",
    "    except ValueError as e:\n",
    "        logging.info(f\"Error: Could not parse JSON. {e}\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_csv_to_dataframe(file_path, dtypes):\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a DataFrame with specified data types for columns.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    dtypes (dict): Dictionary mapping column names to their data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the CSV data with specified data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'pay_date': columns[0],\n",
    "            'total': columns[1],\n",
    "            'user_id': columns[2],\n",
    "            'value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An error occurred while reading the CSV file: {e}\")\n",
    "\n",
    "def aggregate_data(df, df_prints_last_week, date_col, rename_col = None):\n",
    "    \"\"\"\n",
    "    Aggregate payments and total spent in the last three weeks for each user and value proposition.\n",
    "\n",
    "    Parameters:\n",
    "    - df_pays (DataFrame): The DataFrame containing payment data.\n",
    "    - df_prints_last_week (DataFrame): DataFrame containing data for the last week.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Aggregated DataFrame containing payments_3w and total_spent_3w.\n",
    "    \"\"\"\n",
    "    df_merged = df.merge(df_prints_last_week[['user_id', 'value_prop', 'print_date']],\n",
    "                                   on=['user_id', 'value_prop'],\n",
    "                                   how='left')\n",
    "    df_merged = df_merged.dropna(how='any')\n",
    "    df_filtered = df_merged[(df_merged[date_col] >= df_merged['print_date'] - pd.DateOffset(weeks=3)) &\n",
    "                                      (df_merged[date_col] < df_merged['print_date'])]\n",
    "    if rename_col:\n",
    "        logging.info(f\"Aggregating data based on {rename_col} in the last three weeks\")\n",
    "        aggregated_df = df_filtered.groupby(['user_id', 'value_prop','print_date']).size().reset_index(name=rename_col)\n",
    "    else:\n",
    "        logging.info(f\"Aggregating data based on payments_3w and total_spent_3w in the last three weeks\")\n",
    "        aggregated_df = df_filtered.groupby(['user_id', 'value_prop', 'print_date']).agg(\n",
    "            payments_3w=('total', 'size'), total_spent_3w=('total', 'sum')).reset_index()\n",
    "    \n",
    "    return aggregated_df\n",
    "\n",
    "def process_and_save_dataframe(df_prints, df_taps, df_pays, output_csv_path):\n",
    "    \"\"\"\n",
    "    Process and save a DataFrame based on prints, taps, and pays data.\n",
    "\n",
    "    Parameters:\n",
    "    - df_prints (DataFrame): DataFrame containing print data.\n",
    "    - df_taps (DataFrame): DataFrame containing tap data.\n",
    "    - df_pays (DataFrame): DataFrame containing pay data.\n",
    "    - output_csv_path (str): Path to save the processed DataFrame as a CSV.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting data processing\")\n",
    "\n",
    "        logging.info(\"Stripping whitespace from 'value_prop' columns\")\n",
    "        df_prints['value_prop'] = df_prints['value_prop'].str.strip()\n",
    "        df_taps['value_prop'] = df_taps['value_prop'].str.strip()\n",
    "        df_pays['value_prop'] = df_pays['value_prop'].str.strip()\n",
    "\n",
    "        logging.info(\"Calculating the last print date and last week start date\")\n",
    "        last_print_date = df_prints['day'].max()\n",
    "        last_week_start = last_print_date - pd.DateOffset(weeks=1)\n",
    "\n",
    "        logging.info(\"Filtering prints for the last week\")\n",
    "        df_prints_last_week = df_prints[df_prints['day'] >= last_week_start]\n",
    "\n",
    "        logging.info(\"Merging taps data with prints to add clicked column\")\n",
    "        df_prints_last_week = df_prints_last_week.merge(\n",
    "            df_taps[['user_id', 'value_prop']].drop_duplicates(),\n",
    "            on=['user_id', 'value_prop'],\n",
    "            how='left',\n",
    "            indicator=True\n",
    "        )\n",
    "        df_prints_last_week['clicked'] = df_prints_last_week['_merge'] == 'both'\n",
    "        df_prints_last_week = df_prints_last_week.dropna(how='any')\n",
    "        \n",
    "        logging.info(\"Adding columns print_date to df_prints_last_week\")\n",
    "        df_prints_last_week['print_date'] = df_prints_last_week['day']\n",
    "\n",
    "        logging.info(\"number of times the user viewed each value prop in the 3 weeks prior to each print.\")\n",
    "        user_value_prop_views = aggregate_data(df_prints, df_prints_last_week, 'day', 'views_3w')\n",
    "\n",
    "        logging.info(\"number of times the user clicked each value prop in the 3 weeks prior to each print.\")\n",
    "        user_value_prop_clicks = aggregate_data(df_taps, df_prints_last_week, 'day', 'clicks_3w')\n",
    "\n",
    "        logging.info(\"number of payments that the user made for each value prop in the 3 weeks prior to each print.\")\n",
    "        user_value_prop_pays = aggregate_data(df_pays, df_prints_last_week,'pay_date')\n",
    "\n",
    "        logging.info(\"Merging aggregated data with prints\")\n",
    "        df_prints_last_week = df_prints_last_week.merge(user_value_prop_views, how='left', on=['user_id', 'value_prop','print_date'])\n",
    "        df_prints_last_week = df_prints_last_week.merge(user_value_prop_clicks, how='left', on=['user_id', 'value_prop','print_date'])\n",
    "        df_prints_last_week = df_prints_last_week.merge(user_value_prop_pays, how='left', on=['user_id', 'value_prop','print_date'])\n",
    "\n",
    "        logging.info(\"Selecting required columns and filling NAs with 0\")\n",
    "        df_prints_last_week = df_prints_last_week[[\n",
    "            'user_id', 'day', 'value_prop', 'clicked', 'views_3w', 'clicks_3w', 'payments_3w', 'total_spent_3w']]\n",
    "        df_prints_last_week.fillna(0, inplace=True)\n",
    "        df_prints_last_week['total_spent_3w'] = df_prints_last_week['total_spent_3w'].round(4)\n",
    "\n",
    "        logging.info(f\"Saving processed DataFrame to {output_csv_path}\")\n",
    "        df_prints_last_week = df_prints_last_week.sort_values(by=['user_id', 'value_prop'])\n",
    "\n",
    "        for col, dtype in DTYPE_EXPECTED_RESULTS.items():\n",
    "            df_prints_last_week[col] = df_prints_last_week[col].astype(dtype)\n",
    "            \n",
    "        df_prints_last_week.to_csv(output_csv_path, index=False)\n",
    "        logging.info(\"Processed DataFrame saved\")\n",
    "\n",
    "        return df_prints_last_week\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"Reading prints.json\")\n",
    "        df_prints = read_json_lines_to_dataframe(PRINTS_PATH, DTYPE_PRINTS)\n",
    "        \n",
    "        logging.info(\"Reading taps.json\")\n",
    "        df_taps = read_json_lines_to_dataframe(TAPS_PATH, DTYPE_TAPS)\n",
    "        \n",
    "        logging.info(\"Reading pays.csv\")\n",
    "        df_pays = read_csv_to_dataframe(PAYS_PATH, DTYPE_PAYS)\n",
    "\n",
    "        logging.info(\"Transform data and saving output dataset in output3.csv\")\n",
    "        df_expected_result = process_and_save_dataframe(df_prints, df_taps, df_pays, EXPECTED_RESULT_PATH)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    END_TIME = datetime.now()\n",
    "    logging.info(f\"End Time : {END_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "    logging.info(f\"Duration: {END_TIME - START_TIME}\")\n",
    "    logging.info('PROCESS_EXECUTED_SUCCESSFULLY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301336c-20f1-49c2-a177-d3e01b0af2b5",
   "metadata": {},
   "source": [
    "####  Manual testing to verify per user that expected_result.csv is ok , using the original files prints.json, taps.json, and pays.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ce4332-a8c5-451f-8f91-d962d78f3bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 21:15:57,418 - INFO - Libraries calls ok\n",
      "2024-06-10 21:15:57,422 - INFO - Start Time : 20240610_211557 \n",
      "2024-06-10 21:16:04,073 - INFO - Prints of last week for user_id = 1 \n",
      "       day  user_id  position         value_prop\n",
      "2020-11-23        1         2 cellphone_recharge\n",
      "2020-11-30        1         2 cellphone_recharge\n",
      "2020-11-30        1         3         link_cobro\n",
      "2020-11-30        1         1              point\n",
      "2020-11-23        1         0         send_money\n",
      "2020-11-23        1         1          transport\n",
      "2020-11-30        1         0          transport \n",
      "\n",
      "2024-06-10 21:16:04,078 - INFO - All prints for user_id = 1 \n",
      "       day  user_id  position         value_prop\n",
      "2020-11-30        1         2 cellphone_recharge\n",
      "2020-11-23        1         2 cellphone_recharge\n",
      "2020-11-03        1         3   credits_consumer\n",
      "2020-11-14        1         2   credits_consumer\n",
      "2020-11-30        1         3         link_cobro\n",
      "2020-11-03        1         1         link_cobro\n",
      "2020-11-12        1         0         link_cobro\n",
      "2020-11-14        1         0         link_cobro\n",
      "2020-11-30        1         1              point\n",
      "2020-11-17        1         1              point\n",
      "2020-11-14        1         1            prepaid\n",
      "2020-11-03        1         0         send_money\n",
      "2020-11-23        1         0         send_money\n",
      "2020-11-30        1         0          transport\n",
      "2020-11-03        1         2          transport\n",
      "2020-11-23        1         1          transport\n",
      "2020-11-17        1         0          transport \n",
      "\n",
      "2024-06-10 21:16:04,084 - INFO - All clicks for user_id = 1 \n",
      "       day  user_id  position value_prop\n",
      "2020-11-03        1         1 link_cobro \n",
      "\n",
      "2024-06-10 21:16:04,093 - INFO - All payments for user_id = 1 \n",
      "  pay_date  total  user_id         value_prop\n",
      "2020-11-28  15.47        1 cellphone_recharge\n",
      "2020-11-05  37.92        1   credits_consumer\n",
      "2020-11-28 137.14        1         link_cobro\n",
      "2020-11-05 100.89        1          transport \n",
      "\n",
      "2024-06-10 21:16:04,199 - INFO - End Time : 20240610_211604 \n",
      "2024-06-10 21:16:04,200 - INFO - Duration: 0:00:06.777457\n",
      "2024-06-10 21:16:04,201 - INFO - PROCESS_EXECUTED_SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "DTYPE_PRINTS = {\n",
    "    'day': 'datetime64[ns]',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_TAPS = {\n",
    "    'day': 'datetime64[ns]',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_PAYS = {\n",
    "    'pay_date': 'datetime64[ns]',\n",
    "    'total': 'float',\n",
    "    'user_id': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "PRINTS_PATH = 'prints.json'\n",
    "TAPS_PATH = 'taps.json'\n",
    "PAYS_PATH = 'pays.csv'\n",
    "\n",
    "USER_ID_TO_TEST = 1\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Libraries calls ok\")\n",
    "START_TIME = datetime.now()\n",
    "logging.info(f\"Start Time : {START_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "\n",
    "def read_json_lines_to_dataframe(json_lines_path, dtypes):\n",
    "    \"\"\"\n",
    "    Builds a pandas DataFrame from JSON lines with specified column names and data types.\n",
    "    \n",
    "    Parameters:\n",
    "    json_lines_path (str): Path to a file containing JSON lines.\n",
    "    dtypes (dict): Dictionary mapping column names to their expected data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the specified columns and data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_lines_path, 'r') as file:\n",
    "            json_lines_str = file.read()\n",
    "\n",
    "        json_lines = [json.loads(line) for line in json_lines_str.strip().split('\\n')]\n",
    "        df = pd.json_normalize(json_lines)\n",
    "\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'day': columns[0],\n",
    "            'user_id': columns[1],\n",
    "            'event_data.position': columns[2],\n",
    "            'event_data.value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.info(f\"Error: The file at {json_lines_path} was not found.\")\n",
    "    except ValueError as e:\n",
    "        logging.info(f\"Error: Could not parse JSON. {e}\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_csv_to_dataframe(file_path, dtypes):\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a DataFrame with specified data types for columns.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    dtypes (dict): Dictionary mapping column names to their data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the CSV data with specified data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'pay_date': columns[0],\n",
    "            'total': columns[1],\n",
    "            'user_id': columns[2],\n",
    "            'value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An error occurred while reading the CSV file: {e}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df_prints = read_json_lines_to_dataframe(PRINTS_PATH, DTYPE_PRINTS)\n",
    "        df_taps = read_json_lines_to_dataframe(TAPS_PATH, DTYPE_TAPS)\n",
    "        df_pays = read_csv_to_dataframe(PAYS_PATH, DTYPE_PAYS)\n",
    "\n",
    "        df_prints_last_week = df_prints[df_prints['day'] >= df_prints['day'].max() - pd.DateOffset(weeks=1)]\n",
    "\n",
    "        logging.info(f\"Prints of last week for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_prints_last_week[df_prints_last_week['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "\n",
    "        logging.info(f\"All prints for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_prints[df_prints['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "\n",
    "        logging.info(f\"All clicks for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_taps[df_taps['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "\n",
    "        logging.info(f\"All payments for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_pays[df_pays['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    END_TIME = datetime.now()\n",
    "    logging.info(f\"End Time : {END_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "    logging.info(f\"Duration: {END_TIME - START_TIME}\")\n",
    "    logging.info('PROCESS_EXECUTED_SUCCESSFULLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7733b-f606-4801-b1ed-1c8811c78ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
