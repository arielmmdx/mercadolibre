{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceab8d77-808d-4912-8ea6-3dc840e5c300",
   "metadata": {},
   "source": [
    "# Install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95d7d67-7e31-469d-a7a1-16c733d292b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/glue_user/.local/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/glue_user/.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/glue_user/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/glue_user/.local/lib/python3.10/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/glue_user/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694f083-7b51-4e4a-922e-b3a29a98e4ea",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "259aca0a-1d2d-48fe-84cd-3753844b12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 19:32:34,470 - INFO - Libraries calls ok\n",
      "2024-06-10 19:32:34,471 - INFO - Start Time : 20240610_193234 \n",
      "2024-06-10 19:32:34,475 - INFO - Reading prints.json\n",
      "2024-06-10 19:32:39,950 - INFO - Reading taps.json\n",
      "2024-06-10 19:32:40,475 - INFO - Reading pays.csv\n",
      "2024-06-10 19:32:40,792 - INFO - Transform data and saving output dataset in output3.csv\n",
      "2024-06-10 19:32:40,793 - INFO - Starting data processing\n",
      "2024-06-10 19:32:40,794 - INFO - Stripping whitespace from 'value_prop' columns\n",
      "2024-06-10 19:32:41,131 - INFO - Converting date columns to datetime and normalizing\n",
      "2024-06-10 19:32:41,440 - INFO - Calculating the last print date and last week start date\n",
      "2024-06-10 19:32:41,443 - INFO - Filtering prints for the last week\n",
      "2024-06-10 19:32:41,457 - INFO - Merging taps data with prints to add clicked column\n",
      "2024-06-10 19:32:41,577 - INFO - Adding columns print_date to df_prints_last_week\n",
      "2024-06-10 19:32:41,579 - INFO - number of times the user viewed each value prop in the 3 weeks prior to each print.\n",
      "2024-06-10 19:32:41,947 - INFO - Aggregating views in the last three weeks\n",
      "2024-06-10 19:32:41,989 - INFO - number of times the user clicked each value prop in the 3 weeks prior to each print.\n",
      "2024-06-10 19:32:42,070 - INFO - Aggregating clicks in the last three weeks\n",
      "2024-06-10 19:32:42,087 - INFO - number of payments that the user made for each value prop in the 3 weeks prior to each print.\n",
      "2024-06-10 19:32:42,455 - INFO - Aggregating payments and total spent in the last three weeks\n",
      "2024-06-10 19:32:42,501 - INFO - Merging aggregated data with prints\n",
      "2024-06-10 19:32:42,676 - INFO - Selecting required columns and filling NAs with 0\n",
      "2024-06-10 19:32:42,718 - INFO - Saving processed DataFrame to expected_result.csv\n",
      "2024-06-10 19:32:44,673 - INFO - Processed DataFrame saved\n",
      "2024-06-10 19:32:44,715 - INFO - End Time : 20240610_193244 \n",
      "2024-06-10 19:32:44,716 - INFO - Duration: 0:00:10.243925\n",
      "2024-06-10 19:32:44,717 - INFO - PROCESS_EXECUTED_SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "DTYPE_PRINTS = {\n",
    "    'day': 'str',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_TAPS = {\n",
    "    'day': 'str',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_PAYS = {\n",
    "    'pay_date': 'str',\n",
    "    'total': 'float',\n",
    "    'user_id': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_EXPECTED_RESULTS = {\n",
    "    'user_id': 'int',\n",
    "    'day': 'datetime64[ns]',\n",
    "    'value_prop': 'str',\n",
    "    'clicked': 'bool',\n",
    "    'views_3w': 'int',\n",
    "    'clicks_3w': 'int',\n",
    "    'payments_3w': 'int',\n",
    "    'total_spent_3w': 'float'\n",
    "}\n",
    "\n",
    "PRINTS_PATH = 'prints.json'\n",
    "TAPS_PATH = 'taps.json'\n",
    "PAYS_PATH = 'pays.csv'\n",
    "EXPECTED_RESULT_PATH = \"expected_result.csv\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Libraries calls ok\")\n",
    "START_TIME = datetime.now()\n",
    "logging.info(f\"Start Time : {START_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "\n",
    "def read_json_lines_to_dataframe(json_lines_path, dtypes):\n",
    "    \"\"\"\n",
    "    Builds a pandas DataFrame from JSON lines with specified column names and data types.\n",
    "    \n",
    "    Parameters:\n",
    "    json_lines_path (str): Path to a file containing JSON lines.\n",
    "    dtypes (dict): Dictionary mapping column names to their expected data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the specified columns and data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_lines_path, 'r') as file:\n",
    "            json_lines_str = file.read()\n",
    "\n",
    "        json_lines = [json.loads(line) for line in json_lines_str.strip().split('\\n')]\n",
    "        df = pd.json_normalize(json_lines)\n",
    "\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'day': columns[0],\n",
    "            'user_id': columns[1],\n",
    "            'event_data.position': columns[2],\n",
    "            'event_data.value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.info(f\"Error: The file at {json_lines_path} was not found.\")\n",
    "    except ValueError as e:\n",
    "        logging.info(f\"Error: Could not parse JSON. {e}\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_csv_to_dataframe(file_path, dtypes):\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a DataFrame with specified data types for columns.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    dtypes (dict): Dictionary mapping column names to their data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the CSV data with specified data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'pay_date': columns[0],\n",
    "            'total': columns[1],\n",
    "            'user_id': columns[2],\n",
    "            'value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An error occurred while reading the CSV file: {e}\")\n",
    "\n",
    "def process_and_save_dataframe(df_prints, df_taps, df_pays, output_csv_path):\n",
    "    \"\"\"Process the input DataFrames to calculate various metrics for user interactions and save the processed data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df_prints (pandas.DataFrame): DataFrame containing print data.\n",
    "        df_taps (pandas.DataFrame): DataFrame containing tap data.\n",
    "        df_pays (pandas.DataFrame): DataFrame containing pay data.\n",
    "        output_csv_path (str): Path to save the processed data CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed DataFrame containing calculated metrics.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Strips whitespace from 'value_prop' columns in all input DataFrames.\n",
    "    2. Converts date columns to datetime and normalizes them.\n",
    "    3. Calculates the last print date and the start date of the last week.\n",
    "    4. Filters prints for the last week based on the last week start date.\n",
    "    5. Merges taps data with prints to add a 'clicked' column.\n",
    "    6. Merges taps and pays data with prints.\n",
    "    7. Filters prints, taps, and pays data for the last three weeks.\n",
    "    8. Aggregates views, clicks, payments, and total spent in the last three weeks.\n",
    "    9. Merges aggregated data with prints.\n",
    "    10. Selects required columns and fills NA values with 0.\n",
    "    11. Converts 'user_id' to int, 'value_prop' to str, 'clicked' to bool, 'views_3w' and 'clicks_3w' to int,\n",
    "        'payments_3w' to int, and 'total_spent_3w' to float.\n",
    "    12. Rounds 'total_spent_3w' to 4 decimal places.\n",
    "    13. Saves the processed DataFrame to the specified CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting data processing\")\n",
    "\n",
    "        logging.info(\"Stripping whitespace from 'value_prop' columns\")\n",
    "        df_prints['value_prop'] = df_prints['value_prop'].str.strip()\n",
    "        df_taps['value_prop'] = df_taps['value_prop'].str.strip()\n",
    "        df_pays['value_prop'] = df_pays['value_prop'].str.strip()\n",
    "\n",
    "        logging.info(\"Converting date columns to datetime and normalizing\")\n",
    "        df_prints['day'] = pd.to_datetime(df_prints['day']).dt.normalize()\n",
    "        df_taps['day'] = pd.to_datetime(df_taps['day']).dt.normalize()\n",
    "        df_pays['pay_date'] = pd.to_datetime(df_pays['pay_date']).dt.normalize()\n",
    "\n",
    "        logging.info(\"Calculating the last print date and last week start date\")\n",
    "        last_print_date = df_prints['day'].max()\n",
    "        last_week_start = last_print_date - pd.DateOffset(weeks=1)\n",
    "\n",
    "        logging.info(\"Filtering prints for the last week\")\n",
    "        df_prints_last_week = df_prints[df_prints['day'] >= last_week_start]\n",
    "\n",
    "        logging.info(\"Merging taps data with prints to add clicked column\")\n",
    "        df_prints_last_week = df_prints_last_week.merge(\n",
    "            df_taps[['user_id', 'value_prop']].drop_duplicates(),\n",
    "            on=['user_id', 'value_prop'],\n",
    "            how='left',\n",
    "            indicator=True\n",
    "        )\n",
    "        df_prints_last_week['clicked'] = df_prints_last_week['_merge'] == 'both'\n",
    "        df_prints_last_week = df_prints_last_week.dropna(how='any')\n",
    "        \n",
    "        logging.info(\"Adding columns print_date to df_prints_last_week\")\n",
    "        df_prints_last_week['print_date'] = df_prints_last_week['day']\n",
    "\n",
    "        logging.info(\"number of times the user viewed each value prop in the 3 weeks prior to each print.\")\n",
    "        df_prints_merged = df_prints.merge(df_prints_last_week[['user_id', 'value_prop', 'print_date']],\n",
    "                                        on=['user_id', 'value_prop'], \n",
    "                                        how='left')\n",
    "        df_prints_merged = df_prints_merged.dropna(how='any')\n",
    "        df_prints_filtered = df_prints_merged[(df_prints_merged['day'] >= df_prints_merged['print_date'] - pd.DateOffset(weeks=3)) &\n",
    "                                             (df_prints_merged['day'] < df_prints_merged['print_date'])][['user_id','day','print_date','value_prop']]\n",
    "        logging.info(\"Aggregating views in the last three weeks\")\n",
    "        user_value_prop_views = df_prints_filtered.groupby(\n",
    "            ['user_id', 'value_prop','print_date']).size().reset_index(name='views_3w')\n",
    "\n",
    "        logging.info(\"number of times the user clicked each value prop in the 3 weeks prior to each print.\")\n",
    "        df_taps_merged = df_taps.merge(df_prints_last_week[['user_id', 'value_prop', 'print_date']],\n",
    "                                        on=['user_id', 'value_prop'], \n",
    "                                        how='left')\n",
    "        df_taps_merged = df_taps_merged.dropna(how='any')\n",
    "        df_taps_filtered = df_taps_merged[(df_taps_merged['day'] >= df_taps_merged['print_date'] - pd.DateOffset(weeks=3)) &\n",
    "                                           (df_taps_merged['day'] < df_taps_merged['print_date'])][['user_id','print_date','value_prop']]\n",
    "        logging.info(\"Aggregating clicks in the last three weeks\")\n",
    "        user_value_prop_clicks = df_taps_filtered.groupby(\n",
    "            ['user_id', 'value_prop','print_date']).size().reset_index(name='clicks_3w')\n",
    "\n",
    "        logging.info(\"number of payments that the user made for each value prop in the 3 weeks prior to each print.\")\n",
    "        df_pays_merged = df_pays.merge(df_prints_last_week[['user_id', 'value_prop', 'print_date']], \n",
    "                                        on=['user_id', 'value_prop'], \n",
    "                                        how='left')\n",
    "        df_pays_merged = df_pays_merged.dropna(how='any')\n",
    "        df_pays_filtered = df_pays_merged[(df_pays_merged['pay_date'] >= df_pays_merged['print_date'] - pd.DateOffset(weeks=3)) &\n",
    "                                       (df_pays_merged['pay_date'] < df_pays_merged['print_date'])]\n",
    "        logging.info(\"Aggregating payments and total spent in the last three weeks\")\n",
    "        user_value_prop_pays = df_pays_filtered.groupby(['user_id', 'value_prop','print_date']).agg(\n",
    "            payments_3w=('total', 'size'), total_spent_3w=('total', 'sum')).reset_index()\n",
    "\n",
    "        logging.info(\"Merging aggregated data with prints\")\n",
    "        df_prints_last_week = df_prints_last_week.merge(user_value_prop_views, how='left', on=['user_id', 'value_prop','print_date'])\n",
    "        df_prints_last_week = df_prints_last_week.merge(user_value_prop_clicks, how='left', on=['user_id', 'value_prop','print_date'])\n",
    "        df_prints_last_week = df_prints_last_week.merge(user_value_prop_pays, how='left', on=['user_id', 'value_prop','print_date'])\n",
    "\n",
    "        logging.info(\"Selecting required columns and filling NAs with 0\")\n",
    "        df_prints_last_week = df_prints_last_week[[\n",
    "            'user_id', 'day', 'value_prop', 'clicked', 'views_3w', 'clicks_3w', 'payments_3w', 'total_spent_3w']]\n",
    "        df_prints_last_week.fillna(0, inplace=True)\n",
    "        df_prints_last_week['total_spent_3w'] = df_prints_last_week['total_spent_3w'].round(4)\n",
    "\n",
    "        logging.info(f\"Saving processed DataFrame to {output_csv_path}\")\n",
    "        df_prints_last_week = df_prints_last_week.sort_values(by=['user_id', 'value_prop'])\n",
    "        dtype_mapping = {\n",
    "            'user_id': int,\n",
    "            'day': 'datetime64[ns]',\n",
    "            'value_prop': str,\n",
    "            'clicked': bool,\n",
    "            'views_3w': int,\n",
    "            'clicks_3w': int,\n",
    "            'payments_3w': int,\n",
    "            'total_spent_3w': float\n",
    "        }\n",
    "\n",
    "        for col, dtype in DTYPE_EXPECTED_RESULTS.items():\n",
    "            df_prints_last_week[col] = df_prints_last_week[col].astype(dtype)\n",
    "            \n",
    "        df_prints_last_week.to_csv(output_csv_path, index=False)\n",
    "        logging.info(\"Processed DataFrame saved\")\n",
    "\n",
    "        return df_prints_last_week\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"Reading prints.json\")\n",
    "        df_prints = read_json_lines_to_dataframe(PRINTS_PATH, DTYPE_PRINTS)\n",
    "        \n",
    "        logging.info(\"Reading taps.json\")\n",
    "        df_taps = read_json_lines_to_dataframe(TAPS_PATH, DTYPE_TAPS)\n",
    "        \n",
    "        logging.info(\"Reading pays.csv\")\n",
    "        df_pays = read_csv_to_dataframe(PAYS_PATH, DTYPE_PAYS)\n",
    "\n",
    "        logging.info(\"Transform data and saving output dataset in output3.csv\")\n",
    "        df_expected_result = process_and_save_dataframe(df_prints, df_taps, df_pays, expected_result_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    END_TIME = datetime.now()\n",
    "    logging.info(f\"End Time : {END_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "    logging.info(f\"Duration: {END_TIME - START_TIME}\")\n",
    "    logging.info('PROCESS_EXECUTED_SUCCESSFULLY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301336c-20f1-49c2-a177-d3e01b0af2b5",
   "metadata": {},
   "source": [
    "####  Manual testing to verify per user that expected_result.csv is ok , using the original files prints.json, taps.json, and pays.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07ce4332-a8c5-451f-8f91-d962d78f3bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 19:32:48,381 - INFO - Libraries calls ok\n",
      "2024-06-10 19:32:48,383 - INFO - Start Time : 20240610_193248 \n",
      "2024-06-10 19:32:55,167 - INFO - Prints of last week for user_id = 1 \n",
      "       day  user_id  position         value_prop\n",
      "2020-11-23        1         2 cellphone_recharge\n",
      "2020-11-30        1         2 cellphone_recharge\n",
      "2020-11-30        1         3         link_cobro\n",
      "2020-11-30        1         1              point\n",
      "2020-11-23        1         0         send_money\n",
      "2020-11-23        1         1          transport\n",
      "2020-11-30        1         0          transport \n",
      "\n",
      "2024-06-10 19:32:55,172 - INFO - All prints for user_id = 1 \n",
      "       day  user_id  position         value_prop\n",
      "2020-11-30        1         2 cellphone_recharge\n",
      "2020-11-23        1         2 cellphone_recharge\n",
      "2020-11-03        1         3   credits_consumer\n",
      "2020-11-14        1         2   credits_consumer\n",
      "2020-11-30        1         3         link_cobro\n",
      "2020-11-03        1         1         link_cobro\n",
      "2020-11-12        1         0         link_cobro\n",
      "2020-11-14        1         0         link_cobro\n",
      "2020-11-30        1         1              point\n",
      "2020-11-17        1         1              point\n",
      "2020-11-14        1         1            prepaid\n",
      "2020-11-03        1         0         send_money\n",
      "2020-11-23        1         0         send_money\n",
      "2020-11-30        1         0          transport\n",
      "2020-11-03        1         2          transport\n",
      "2020-11-23        1         1          transport\n",
      "2020-11-17        1         0          transport \n",
      "\n",
      "2024-06-10 19:32:55,183 - INFO - All clicks for user_id = 1 \n",
      "       day  user_id  position value_prop\n",
      "2020-11-03        1         1 link_cobro \n",
      "\n",
      "2024-06-10 19:32:55,225 - INFO - All payments for user_id = 1 \n",
      "  pay_date  total  user_id         value_prop\n",
      "2020-11-28  15.47        1 cellphone_recharge\n",
      "2020-11-05  37.92        1   credits_consumer\n",
      "2020-11-28 137.14        1         link_cobro\n",
      "2020-11-05 100.89        1          transport \n",
      "\n",
      "2024-06-10 19:32:55,266 - INFO - End Time : 20240610_193255 \n",
      "2024-06-10 19:32:55,268 - INFO - Duration: 0:00:06.883605\n",
      "2024-06-10 19:32:55,268 - INFO - PROCESS_EXECUTED_SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "DTYPE_PRINTS = {\n",
    "    'day': 'str',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_TAPS = {\n",
    "    'day': 'str',\n",
    "    'user_id': 'int',\n",
    "    'position': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "DTYPE_PAYS = {\n",
    "    'pay_date': 'str',\n",
    "    'total': 'float',\n",
    "    'user_id': 'int',\n",
    "    'value_prop': 'str'\n",
    "}\n",
    "\n",
    "PRINTS_PATH = 'prints.json'\n",
    "TAPS_PATH = 'taps.json'\n",
    "PAYS_PATH = 'pays.csv'\n",
    "\n",
    "USER_ID_TO_TEST = 1\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.info(\"Libraries calls ok\")\n",
    "START_TIME = datetime.now()\n",
    "logging.info(f\"Start Time : {START_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "\n",
    "def read_json_lines_to_dataframe(json_lines_path, dtypes):\n",
    "    \"\"\"\n",
    "    Builds a pandas DataFrame from JSON lines with specified column names and data types.\n",
    "    \n",
    "    Parameters:\n",
    "    json_lines_path (str): Path to a file containing JSON lines.\n",
    "    dtypes (dict): Dictionary mapping column names to their expected data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the specified columns and data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_lines_path, 'r') as file:\n",
    "            json_lines_str = file.read()\n",
    "\n",
    "        json_lines = [json.loads(line) for line in json_lines_str.strip().split('\\n')]\n",
    "        df = pd.json_normalize(json_lines)\n",
    "\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'day': columns[0],\n",
    "            'user_id': columns[1],\n",
    "            'event_data.position': columns[2],\n",
    "            'event_data.value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.info(f\"Error: The file at {json_lines_path} was not found.\")\n",
    "    except ValueError as e:\n",
    "        logging.info(f\"Error: Could not parse JSON. {e}\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def read_csv_to_dataframe(file_path, dtypes):\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a DataFrame with specified data types for columns.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    dtypes (dict): Dictionary mapping column names to their data types.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the CSV data with specified data types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        columns = list(dtypes.keys())\n",
    "\n",
    "        column_mapping = {\n",
    "            'pay_date': columns[0],\n",
    "            'total': columns[1],\n",
    "            'user_id': columns[2],\n",
    "            'value_prop': columns[3]\n",
    "        }\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        df = df[columns]\n",
    "        \n",
    "        for col, dtype in dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.info(f\"An error occurred while reading the CSV file: {e}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df_prints = read_json_lines_to_dataframe(PRINTS_PATH, DTYPE_PRINTS)\n",
    "        df_taps = read_json_lines_to_dataframe(TAPS_PATH, DTYPE_TAPS)\n",
    "        df_pays = read_csv_to_dataframe(PAYS_PATH, DTYPE_PAYS)\n",
    "\n",
    "        df_prints['day'] = pd.to_datetime(df_prints['day']).dt.normalize()\n",
    "\n",
    "        df_prints_last_week = df_prints[df_prints['day'] >= df_prints['day'].max() - pd.DateOffset(weeks=1)]\n",
    "\n",
    "        logging.info(f\"Prints of last week for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_prints_last_week[df_prints_last_week['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "\n",
    "        logging.info(f\"All prints for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_prints[df_prints['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "\n",
    "        logging.info(f\"All clicks for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_taps[df_taps['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "\n",
    "        logging.info(f\"All payments for user_id = {USER_ID_TO_TEST} \\n\"\n",
    "             f\"{df_pays[df_pays['user_id'] == USER_ID_TO_TEST].sort_values(by='value_prop').to_string(index=False)} \\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    END_TIME = datetime.now()\n",
    "    logging.info(f\"End Time : {END_TIME.strftime('%Y%m%d_%H%M%S')} \")\n",
    "    logging.info(f\"Duration: {END_TIME - START_TIME}\")\n",
    "    logging.info('PROCESS_EXECUTED_SUCCESSFULLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7733b-f606-4801-b1ed-1c8811c78ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
